{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0dff1b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8901e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import gensim.downloader\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434a0e5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e8a198-92c0-4eb3-bc62-435c35a70434",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "\n",
    "def word_tokenize(s: str):\n",
    "    return [x.lower() for x in word_tokenize_pattern.findall(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e61b3",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3292a6-f7ab-4858-9732-76ba312fe93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Relaxed\", \"Sad\"]\n",
    "\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    # overall\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"=== Overall (Macro Avg) ===\")\n",
    "    print(\"Precision:\", p)\n",
    "    print(\"Recall:\", r)\n",
    "    print(\"F1:\", f)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print()\n",
    "\n",
    "    # Per-emotion metrics\n",
    "    p_i, r_i, f_i, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"=== Per Emotion (Class) Metrics ===\")\n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        print(f\"{emotion}:\")\n",
    "        print(\"  Precision:\", p_i[i])\n",
    "        print(\"  Recall:   \", r_i[i])\n",
    "        print(\"  F1:       \", f_i[i])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f, \"accuracy\": acc}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4b6e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edfab4b-46e2-464a-a47f-a2bb86f62c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"NJU_MusicMood_v1.0\"\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "timestamp_pattern = re.compile(r\"\\[\\d{2}:\\d{2}(?:\\.\\d{2})?\\]\")\n",
    "\n",
    "\n",
    "def clean_lyrics(text: str) -> str:\n",
    "    # Remove timestamps\n",
    "    text = timestamp_pattern.sub(\"\", text)\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    lines = [line for line in lines if line]  # drop empty lines\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def get_lyrics(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    return clean_lyrics(raw)\n",
    "\n",
    "\n",
    "def get_lyrics_and_labels(split: str):\n",
    "    texts, labels = [], []\n",
    "    for emotion in EMOTIONS:\n",
    "        folder = os.path.join(DATASET_DIR, emotion, split)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder):\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "            if fname.lower() == \"info.txt\":\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(folder, fname)\n",
    "            txt = get_lyrics(path)\n",
    "            if txt.strip():\n",
    "                texts.append(txt)\n",
    "                labels.append(emotion)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# Load data \n",
    "train_texts, train_labels = get_lyrics_and_labels(\"Train\")\n",
    "dev_texts, dev_labels = get_lyrics_and_labels(\"Test\")\n",
    "\n",
    "assert len(train_texts) == len(train_labels)\n",
    "assert len(dev_texts) == len(dev_labels)\n",
    "\n",
    "# Datasets\n",
    "train_ds = Dataset.from_dict(\n",
    "    {\"text\": train_texts, \"label\": [label2id[l] for l in train_labels]}\n",
    ")\n",
    "dev_ds = Dataset.from_dict(\n",
    "    {\"text\": dev_texts, \"label\": [label2id[l] for l in dev_labels]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93414e",
   "metadata": {},
   "source": [
    "## Baseline: Bag of Words & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a40396-55f3-43dc-af8e-fd45ad82c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: Bag of Words & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3826099694894021\n",
      "Recall: 0.3761659284450901\n",
      "F1: 0.3761515331158733\n",
      "Accuracy: 0.3660477453580902\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5\n",
      "  Recall:    0.5070422535211268\n",
      "  F1:        0.5034965034965035\n",
      "Happy:\n",
      "  Precision: 0.4567901234567901\n",
      "  Recall:    0.3490566037735849\n",
      "  F1:        0.39572192513368987\n",
      "Relaxed:\n",
      "  Precision: 0.3076923076923077\n",
      "  Recall:    0.39603960396039606\n",
      "  F1:        0.3463203463203463\n",
      "Sad:\n",
      "  Precision: 0.26595744680851063\n",
      "  Recall:    0.25252525252525254\n",
      "  F1:        0.25906735751295334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline: Bag of Words & Logistic Regression ===\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=word_tokenize)\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "dev_counts = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "lr_bow = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_bow.fit(train_counts, train_labels)\n",
    "\n",
    "lr_bow_dev_predictions = lr_bow.predict(dev_counts)\n",
    "print_results(dev_labels, lr_bow_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bab194",
   "metadata": {},
   "source": [
    "## Baseline 2: Word2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e417a9d1-ef6c-4527-a6dc-49d0e591d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4596175291565\n",
      "Recall: 0.47568620468212114\n",
      "F1: 0.45246239781207337\n",
      "Accuracy: 0.4509283819628647\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6063829787234043\n",
      "  Recall:    0.8028169014084507\n",
      "  F1:        0.6909090909090909\n",
      "Happy:\n",
      "  Precision: 0.47572815533980584\n",
      "  Recall:    0.46226415094339623\n",
      "  F1:        0.4688995215311005\n",
      "Relaxed:\n",
      "  Precision: 0.3308270676691729\n",
      "  Recall:    0.43564356435643564\n",
      "  F1:        0.37606837606837606\n",
      "Sad:\n",
      "  Precision: 0.425531914893617\n",
      "  Recall:    0.20202020202020202\n",
      "  F1:        0.273972602739726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec & Logistic Regression ===\")\n",
    "\n",
    "w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "VECTOR_SIZE = w2v_model.vector_size\n",
    "\n",
    "\n",
    "def vec_for_doc(tokenized_doc):\n",
    "    vectors = [w2v_model[word] for word in tokenized_doc if word in w2v_model.key_to_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(VECTOR_SIZE, dtype=\"float32\")\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "train_vecs = [vec_for_doc(word_tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_texts]\n",
    "\n",
    "lr_w2v = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_w2v.fit(train_vecs, train_labels)\n",
    "\n",
    "w2v_dev_predictions = lr_w2v.predict(dev_vecs)\n",
    "print_results(dev_labels, w2v_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eee90",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8275c4c9-0c1f-4969-a64a-e055131ce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length: int = 256):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(_tok, batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def train_and_eval_transformer(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    dev_dataset: Dataset,\n",
    "    output_dir: str,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    train_bs: int,\n",
    "    eval_bs: int,\n",
    "    set_pad_token_eos: bool = False,\n",
    "):\n",
    "    print(f\"=== Fine-tuning {model_name} ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenized_train = tokenize_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenize_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(EMOTIONS),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=16,\n",
    "        log_level=\"error\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"{model_name} dev results:\", eval_results)\n",
    "\n",
    "    pred_output = trainer.predict(tokenized_dev)\n",
    "    logits = pred_output.predictions\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    print(f\"{model_name} classification report:\")\n",
    "    print_results(dev_labels, pred_labels)\n",
    "\n",
    "    return trainer, eval_results, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02eb96",
   "metadata": {},
   "source": [
    "## DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc595940-367c-40d2-96c0-8cc36c91b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilgpt2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 841.22 examples/s]\n",
      "Map: 100%|██████████| 377/377 [00:00<00:00, 1382.43 examples/s]\n",
      "C:\\Users\\samke\\AppData\\Local\\Temp\\ipykernel_23784\\2135660195.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0607, 'grad_norm': 30.447505950927734, 'learning_rate': 4.85e-05, 'epoch': 0.16}\n",
      "{'loss': 1.7591, 'grad_norm': 70.75331115722656, 'learning_rate': 4.69e-05, 'epoch': 0.32}\n",
      "{'loss': 1.7095, 'grad_norm': 41.877601623535156, 'learning_rate': 4.53e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4226, 'grad_norm': 28.678590774536133, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3951, 'grad_norm': 38.81130599975586, 'learning_rate': 4.21e-05, 'epoch': 0.8}\n",
      "{'loss': 1.466, 'grad_norm': 61.95573043823242, 'learning_rate': 4.05e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 1.3817614316940308, 'eval_precision': 0.4059052922244166, 'eval_recall': 0.3381907749637573, 'eval_f1': 0.25614746543778805, 'eval_accuracy': 0.2917771883289125, 'eval_runtime': 179.307, 'eval_samples_per_second': 2.103, 'eval_steps_per_second': 0.53, 'epoch': 1.0}\n",
      "{'loss': 1.2712, 'grad_norm': 22.22853660583496, 'learning_rate': 3.8900000000000004e-05, 'epoch': 1.12}\n",
      "{'loss': 1.278, 'grad_norm': 40.40504837036133, 'learning_rate': 3.73e-05, 'epoch': 1.28}\n",
      "{'loss': 1.1296, 'grad_norm': 18.054903030395508, 'learning_rate': 3.57e-05, 'epoch': 1.44}\n",
      "{'loss': 1.2165, 'grad_norm': 19.681936264038086, 'learning_rate': 3.41e-05, 'epoch': 1.6}\n",
      "{'loss': 1.2698, 'grad_norm': 40.98830032348633, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.76}\n",
      "{'loss': 1.2295, 'grad_norm': 35.68042755126953, 'learning_rate': 3.09e-05, 'epoch': 1.92}\n",
      "{'eval_loss': 1.4056869745254517, 'eval_precision': 0.42289689357988325, 'eval_recall': 0.4065403118839654, 'eval_f1': 0.3566713247245236, 'eval_accuracy': 0.3978779840848806, 'eval_runtime': 126.3906, 'eval_samples_per_second': 2.983, 'eval_steps_per_second': 0.752, 'epoch': 2.0}\n",
      "{'loss': 1.0578, 'grad_norm': 50.015342712402344, 'learning_rate': 2.93e-05, 'epoch': 2.08}\n",
      "{'loss': 0.9373, 'grad_norm': 37.942848205566406, 'learning_rate': 2.7700000000000002e-05, 'epoch': 2.24}\n",
      "{'loss': 0.7979, 'grad_norm': 27.703678131103516, 'learning_rate': 2.61e-05, 'epoch': 2.4}\n",
      "{'loss': 0.9093, 'grad_norm': 20.318260192871094, 'learning_rate': 2.45e-05, 'epoch': 2.56}\n",
      "{'loss': 0.869, 'grad_norm': 36.75762939453125, 'learning_rate': 2.29e-05, 'epoch': 2.7199999999999998}\n",
      "{'loss': 0.9124, 'grad_norm': 19.917510986328125, 'learning_rate': 2.13e-05, 'epoch': 2.88}\n",
      "{'eval_loss': 1.3507041931152344, 'eval_precision': 0.4707746577845237, 'eval_recall': 0.4927344315616786, 'eval_f1': 0.4545480321065144, 'eval_accuracy': 0.46419098143236076, 'eval_runtime': 87.3544, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.088, 'epoch': 3.0}\n",
      "{'loss': 0.8017, 'grad_norm': 12.890719413757324, 'learning_rate': 1.97e-05, 'epoch': 3.04}\n",
      "{'loss': 0.5428, 'grad_norm': 28.504844665527344, 'learning_rate': 1.81e-05, 'epoch': 3.2}\n",
      "{'loss': 0.4693, 'grad_norm': 40.70982360839844, 'learning_rate': 1.65e-05, 'epoch': 3.36}\n",
      "{'loss': 0.3887, 'grad_norm': 22.85255241394043, 'learning_rate': 1.49e-05, 'epoch': 3.52}\n",
      "{'loss': 0.5708, 'grad_norm': 44.08671188354492, 'learning_rate': 1.3300000000000001e-05, 'epoch': 3.68}\n",
      "{'loss': 0.5388, 'grad_norm': 18.585424423217773, 'learning_rate': 1.1700000000000001e-05, 'epoch': 3.84}\n",
      "{'loss': 0.4864, 'grad_norm': 31.2594051361084, 'learning_rate': 1.0100000000000002e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 1.5141569375991821, 'eval_precision': 0.4685738487522144, 'eval_recall': 0.49103742423139474, 'eval_f1': 0.4732433582447508, 'eval_accuracy': 0.47214854111405835, 'eval_runtime': 157.1195, 'eval_samples_per_second': 2.399, 'eval_steps_per_second': 0.605, 'epoch': 4.0}\n",
      "{'loss': 0.3145, 'grad_norm': 5.1138997077941895, 'learning_rate': 8.500000000000002e-06, 'epoch': 4.16}\n",
      "{'loss': 0.2989, 'grad_norm': 29.969736099243164, 'learning_rate': 6.900000000000001e-06, 'epoch': 4.32}\n",
      "{'loss': 0.2631, 'grad_norm': 15.664993286132812, 'learning_rate': 5.3e-06, 'epoch': 4.48}\n",
      "{'loss': 0.2422, 'grad_norm': 8.155415534973145, 'learning_rate': 3.7e-06, 'epoch': 4.64}\n",
      "{'loss': 0.3618, 'grad_norm': 42.00629806518555, 'learning_rate': 2.1000000000000002e-06, 'epoch': 4.8}\n",
      "{'loss': 0.2199, 'grad_norm': 17.76459503173828, 'learning_rate': 5.000000000000001e-07, 'epoch': 4.96}\n",
      "{'eval_loss': 1.6245592832565308, 'eval_precision': 0.4610838268573355, 'eval_recall': 0.4760903508634148, 'eval_f1': 0.4636901546389142, 'eval_accuracy': 0.4588859416445623, 'eval_runtime': 141.4489, 'eval_samples_per_second': 2.665, 'eval_steps_per_second': 0.672, 'epoch': 5.0}\n",
      "{'train_runtime': 278807.6255, 'train_samples_per_second': 0.007, 'train_steps_per_second': 0.002, 'train_loss': 0.9041905665397644, 'epoch': 5.0}\n",
      "{'eval_loss': 1.6245592832565308, 'eval_precision': 0.4610838268573355, 'eval_recall': 0.4760903508634148, 'eval_f1': 0.4636901546389142, 'eval_accuracy': 0.4588859416445623, 'eval_runtime': 126.9915, 'eval_samples_per_second': 2.969, 'eval_steps_per_second': 0.748, 'epoch': 5.0}\n",
      "distilgpt2 dev results: {'eval_loss': 1.6245592832565308, 'eval_precision': 0.4610838268573355, 'eval_recall': 0.4760903508634148, 'eval_f1': 0.4636901546389142, 'eval_accuracy': 0.4588859416445623, 'eval_runtime': 126.9915, 'eval_samples_per_second': 2.969, 'eval_steps_per_second': 0.748, 'epoch': 5.0}\n",
      "distilgpt2 classification report:\n",
      "Precision:  0.4610838268573355\n",
      "Recall:  0.4760903508634148\n",
      "F1:  0.4636901546389142\n",
      "Accuracy:  0.4588859416445623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_trainer, gpt2_results, gpt2_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilgpt2\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilgpt2_output\",\n",
    "    num_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4e43",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe4e592e-e7ad-4d23-b757-580470945074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilbert/distilbert-base-uncased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 973.00 examples/s] \n",
      "Map: 100%|██████████| 377/377 [00:00<00:00, 1314.74 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\samke\\AppData\\Local\\Temp\\ipykernel_16584\\2135660195.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 22:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.428900</td>\n",
       "      <td>1.381346</td>\n",
       "      <td>0.283878</td>\n",
       "      <td>0.356637</td>\n",
       "      <td>0.241124</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.169500</td>\n",
       "      <td>1.169916</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.484077</td>\n",
       "      <td>0.421109</td>\n",
       "      <td>0.453581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.011000</td>\n",
       "      <td>1.109788</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.546339</td>\n",
       "      <td>0.533452</td>\n",
       "      <td>0.522546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilbert-base-uncased dev results: {'eval_loss': 1.109787940979004, 'eval_precision': 0.52729114236282, 'eval_recall': 0.5463388486071568, 'eval_f1': 0.5334521378486706, 'eval_accuracy': 0.5225464190981433, 'eval_runtime': 86.7383, 'eval_samples_per_second': 4.346, 'eval_steps_per_second': 1.095, 'epoch': 3.0}\n",
      "distilbert/distilbert-base-uncased classification report:\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.52729114236282\n",
      "Recall: 0.5463388486071568\n",
      "F1: 0.5334521378486706\n",
      "Accuracy: 0.5225464190981433\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6896551724137931\n",
      "  Recall:    0.8450704225352113\n",
      "  F1:        0.759493670886076\n",
      "Happy:\n",
      "  Precision: 0.5555555555555556\n",
      "  Recall:    0.4716981132075472\n",
      "  F1:        0.5102040816326531\n",
      "Relaxed:\n",
      "  Precision: 0.4594594594594595\n",
      "  Recall:    0.504950495049505\n",
      "  F1:        0.4811320754716981\n",
      "Sad:\n",
      "  Precision: 0.4044943820224719\n",
      "  Recall:    0.36363636363636365\n",
      "  F1:        0.3829787234042553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distilbert_trainer, distilbert_results, distilbert_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilbert/distilbert-base-uncased\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilbert_musicmood\",\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
