{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0dff1b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8901e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import gensim.downloader\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434a0e5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5e8a198-92c0-4eb3-bc62-435c35a70434",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "\n",
    "def word_tokenize(s: str):\n",
    "    return [x.lower() for x in word_tokenize_pattern.findall(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e61b3",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb3292a6-f7ab-4858-9732-76ba312fe93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMOTIONS = [\"Angry\", \"Happy\", \"Relaxed\", \"Sad\"]\n",
    "\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    # overall\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"=== Overall (Macro Avg) ===\")\n",
    "    print(\"Precision:\", p)\n",
    "    print(\"Recall:\", r)\n",
    "    print(\"F1:\", f)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print()\n",
    "\n",
    "    # Per-emotion metrics\n",
    "    p_i, r_i, f_i, _ = precision_recall_fscore_support(\n",
    "        gold_labels, predicted_labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"=== Per Emotion (Class) Metrics ===\")\n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        print(f\"{emotion}:\")\n",
    "        print(\"  Precision:\", p_i[i])\n",
    "        print(\"  Recall:   \", r_i[i])\n",
    "        print(\"  F1:       \", f_i[i])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f, \"accuracy\": acc}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b4b6e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4edfab4b-46e2-464a-a47f-a2bb86f62c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"NJU_MusicMood_v1.0\"\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "timestamp_pattern = re.compile(r\"\\[\\d{2}:\\d{2}(?:\\.\\d{2})?\\]\")\n",
    "\n",
    "\n",
    "def clean_lyrics(text: str) -> str:\n",
    "    # Remove timestamps like [00:29]\n",
    "    text = timestamp_pattern.sub(\"\", text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize quotes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "    # Remove ellipses and repeated dots\n",
    "    text = re.sub(r\"\\.{2,}\", \" \", text)\n",
    "\n",
    "    # Remove long underscores\n",
    "    text = re.sub(r\"_{2,}\", \" \", text)\n",
    "\n",
    "    # Remove trailing \"end\" markers at the end of the file\n",
    "    text = re.sub(r\"\\bend[.\\s]*$\", \"\", text.strip())\n",
    "\n",
    "    # Replace newlines with space\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Keep only letters, digits, spaces, apostrophes\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def get_lyrics(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    return clean_lyrics(raw)\n",
    "\n",
    "\n",
    "def get_lyrics_and_labels(split: str):\n",
    "    texts, labels = [], []\n",
    "    for emotion in EMOTIONS:\n",
    "        folder = os.path.join(DATASET_DIR, emotion, split)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder):\n",
    "            if not fname.endswith(\".txt\"):\n",
    "                continue\n",
    "            if fname.lower() == \"info.txt\":\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(folder, fname)\n",
    "            txt = get_lyrics(path)\n",
    "            if txt.strip():\n",
    "                texts.append(txt)\n",
    "                labels.append(emotion)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# Load data \n",
    "train_texts, train_labels = get_lyrics_and_labels(\"Train\")\n",
    "dev_texts, dev_labels = get_lyrics_and_labels(\"Test\")\n",
    "\n",
    "assert len(train_texts) == len(train_labels)\n",
    "assert len(dev_texts) == len(dev_labels)\n",
    "\n",
    "# Datasets\n",
    "train_ds = Dataset.from_dict(\n",
    "    {\"text\": train_texts, \"label\": [label2id[l] for l in train_labels]}\n",
    ")\n",
    "dev_ds = Dataset.from_dict(\n",
    "    {\"text\": dev_texts, \"label\": [label2id[l] for l in dev_labels]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93414e",
   "metadata": {},
   "source": [
    "## Baseline: Bag of Words & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5a40396-55f3-43dc-af8e-fd45ad82c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: Bag of Words & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.38026220704863894\n",
      "Recall: 0.37264480168452674\n",
      "F1: 0.3731436538331033\n",
      "Accuracy: 0.363395225464191\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.49295774647887325\n",
      "  Recall:    0.49295774647887325\n",
      "  F1:        0.49295774647887325\n",
      "Happy:\n",
      "  Precision: 0.4567901234567901\n",
      "  Recall:    0.3490566037735849\n",
      "  F1:        0.39572192513368987\n",
      "Relaxed:\n",
      "  Precision: 0.3053435114503817\n",
      "  Recall:    0.39603960396039606\n",
      "  F1:        0.3448275862068966\n",
      "Sad:\n",
      "  Precision: 0.26595744680851063\n",
      "  Recall:    0.25252525252525254\n",
      "  F1:        0.25906735751295334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline: Bag of Words & Logistic Regression ===\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=word_tokenize)\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "dev_counts = count_vectorizer.transform(dev_texts)\n",
    "\n",
    "lr_bow = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_bow.fit(train_counts, train_labels)\n",
    "\n",
    "lr_bow_dev_predictions = lr_bow.predict(dev_counts)\n",
    "print_results(dev_labels, lr_bow_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bab194",
   "metadata": {},
   "source": [
    "## Baseline 2: Word2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e417a9d1-ef6c-4527-a6dc-49d0e591d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec & Logistic Regression ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.46144573621456286\n",
      "Recall: 0.4781614522068736\n",
      "F1: 0.4553408410787571\n",
      "Accuracy: 0.4535809018567639\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6129032258064516\n",
      "  Recall:    0.8028169014084507\n",
      "  F1:        0.6951219512195121\n",
      "Happy:\n",
      "  Precision: 0.4803921568627451\n",
      "  Recall:    0.46226415094339623\n",
      "  F1:        0.47115384615384615\n",
      "Relaxed:\n",
      "  Precision: 0.3358208955223881\n",
      "  Recall:    0.44554455445544555\n",
      "  F1:        0.3829787234042553\n",
      "Sad:\n",
      "  Precision: 0.4166666666666667\n",
      "  Recall:    0.20202020202020202\n",
      "  F1:        0.272108843537415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec & Logistic Regression ===\")\n",
    "\n",
    "w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "VECTOR_SIZE = w2v_model.vector_size\n",
    "\n",
    "\n",
    "def vec_for_doc(tokenized_doc):\n",
    "    vectors = [w2v_model[word] for word in tokenized_doc if word in w2v_model.key_to_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(VECTOR_SIZE, dtype=\"float32\")\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "train_vecs = [vec_for_doc(word_tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_texts]\n",
    "\n",
    "lr_w2v = LogisticRegression(max_iter=500, random_state=0)\n",
    "lr_w2v.fit(train_vecs, train_labels)\n",
    "\n",
    "w2v_dev_predictions = lr_w2v.predict(dev_vecs)\n",
    "print_results(dev_labels, w2v_dev_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eee90",
   "metadata": {},
   "source": [
    "## Split lyrics into start middle end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39a0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment(text, segment=\"start\", portion=0.3):\n",
    "    \"\"\"\n",
    "    Extract a portion of the lyrics.\n",
    "    portion=0.3 means 30% of tokens.\n",
    "    segment can be \"start\", \"middle\", or \"end\".\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return \"\"\n",
    "\n",
    "    cut = int(n * portion)  # number of tokens for start/end\n",
    "\n",
    "    if segment == \"start\":\n",
    "        return \" \".join(tokens[:cut])\n",
    "\n",
    "    elif segment == \"middle\":\n",
    "        start = int(n * 0.35)\n",
    "        end = int(n * 0.65)\n",
    "        return \" \".join(tokens[start:end])\n",
    "\n",
    "    elif segment == \"end\":\n",
    "        return \" \".join(tokens[-cut:])\n",
    "\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Build dev\n",
    "dev_start_texts = [get_segment(t, \"start\") for t in dev_texts]\n",
    "dev_middle_texts = [get_segment(t, \"middle\") for t in dev_texts]\n",
    "dev_end_texts = [get_segment(t, \"end\") for t in dev_texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c423049",
   "metadata": {},
   "source": [
    "## Evaluate BoW model Segmented Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c265a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BoW Logistic Regression: Position-based Evaluation (NO retraining) ===\n",
      "\n",
      "--- BoW on START segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.41852128751856615\n",
      "Recall: 0.3243719867601961\n",
      "F1: 0.27986468063888137\n",
      "Accuracy: 0.32625994694960214\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5882352941176471\n",
      "  Recall:    0.28169014084507044\n",
      "  F1:        0.38095238095238093\n",
      "Happy:\n",
      "  Precision: 0.6\n",
      "  Recall:    0.11320754716981132\n",
      "  F1:        0.19047619047619047\n",
      "Relaxed:\n",
      "  Precision: 0.29537366548042704\n",
      "  Recall:    0.8217821782178217\n",
      "  F1:        0.43455497382198954\n",
      "Sad:\n",
      "  Precision: 0.19047619047619047\n",
      "  Recall:    0.08080808080808081\n",
      "  F1:        0.11347517730496454\n",
      "\n",
      "\n",
      "--- BoW on MIDDLE segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.40280577696950937\n",
      "Recall: 0.3146209717968874\n",
      "F1: 0.27415579397176265\n",
      "Accuracy: 0.3183023872679045\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5625\n",
      "  Recall:    0.2535211267605634\n",
      "  F1:        0.34951456310679613\n",
      "Happy:\n",
      "  Precision: 0.5384615384615384\n",
      "  Recall:    0.1320754716981132\n",
      "  F1:        0.21212121212121213\n",
      "Relaxed:\n",
      "  Precision: 0.28169014084507044\n",
      "  Recall:    0.7920792079207921\n",
      "  F1:        0.4155844155844156\n",
      "Sad:\n",
      "  Precision: 0.22857142857142856\n",
      "  Recall:    0.08080808080808081\n",
      "  F1:        0.11940298507462686\n",
      "\n",
      "\n",
      "--- BoW on END segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3494493529118965\n",
      "Recall: 0.3074784724021166\n",
      "F1: 0.27927655401044243\n",
      "Accuracy: 0.3129973474801061\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.45714285714285713\n",
      "  Recall:    0.22535211267605634\n",
      "  F1:        0.3018867924528302\n",
      "Happy:\n",
      "  Precision: 0.4146341463414634\n",
      "  Recall:    0.16037735849056603\n",
      "  F1:        0.23129251700680273\n",
      "Relaxed:\n",
      "  Precision: 0.2938775510204082\n",
      "  Recall:    0.7128712871287128\n",
      "  F1:        0.4161849710982659\n",
      "Sad:\n",
      "  Precision: 0.23214285714285715\n",
      "  Recall:    0.13131313131313133\n",
      "  F1:        0.16774193548387098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BoW Logistic Regression: Position-based Evaluation (NO retraining) ===\")\n",
    "\n",
    "# START\n",
    "dev_start_counts = count_vectorizer.transform(dev_start_texts)\n",
    "bow_start_preds = lr_bow.predict(dev_start_counts)\n",
    "print(\"\\n--- BoW on START segment only ---\")\n",
    "print_results(dev_labels, bow_start_preds)\n",
    "\n",
    "# MIDDLE\n",
    "dev_middle_counts = count_vectorizer.transform(dev_middle_texts)\n",
    "bow_middle_preds = lr_bow.predict(dev_middle_counts)\n",
    "print(\"\\n--- BoW on MIDDLE segment only ---\")\n",
    "print_results(dev_labels, bow_middle_preds)\n",
    "\n",
    "# END\n",
    "dev_end_counts = count_vectorizer.transform(dev_end_texts)\n",
    "bow_end_preds = lr_bow.predict(dev_end_counts)\n",
    "print(\"\\n--- BoW on END segment only ---\")\n",
    "print_results(dev_labels, bow_end_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835d866",
   "metadata": {},
   "source": [
    "## Evaluate Word2Vec on Segmented Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0efcd5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec Logistic Regression: Position-based Evaluation (NO retraining) ===\n",
      "\n",
      "--- W2V on START segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4094516545709824\n",
      "Recall: 0.4308409946761351\n",
      "F1: 0.4092941134519704\n",
      "Accuracy: 0.41114058355437666\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5051546391752577\n",
      "  Recall:    0.6901408450704225\n",
      "  F1:        0.5833333333333334\n",
      "Happy:\n",
      "  Precision: 0.43434343434343436\n",
      "  Recall:    0.4056603773584906\n",
      "  F1:        0.4195121951219512\n",
      "Relaxed:\n",
      "  Precision: 0.3464566929133858\n",
      "  Recall:    0.43564356435643564\n",
      "  F1:        0.38596491228070173\n",
      "Sad:\n",
      "  Precision: 0.35185185185185186\n",
      "  Recall:    0.1919191919191919\n",
      "  F1:        0.24836601307189543\n",
      "\n",
      "\n",
      "--- W2V on MIDDLE segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4279684703083266\n",
      "Recall: 0.45646796941187606\n",
      "F1: 0.42801924337609987\n",
      "Accuracy: 0.4323607427055703\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5445544554455446\n",
      "  Recall:    0.7746478873239436\n",
      "  F1:        0.6395348837209303\n",
      "Happy:\n",
      "  Precision: 0.4742268041237113\n",
      "  Recall:    0.4339622641509434\n",
      "  F1:        0.45320197044334976\n",
      "Relaxed:\n",
      "  Precision: 0.34615384615384615\n",
      "  Recall:    0.44554455445544555\n",
      "  F1:        0.38961038961038963\n",
      "Sad:\n",
      "  Precision: 0.3469387755102041\n",
      "  Recall:    0.1717171717171717\n",
      "  F1:        0.22972972972972974\n",
      "\n",
      "\n",
      "--- W2V on END segment only ---\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.3880041048133154\n",
      "Recall: 0.4115178558780672\n",
      "F1: 0.3888729306339611\n",
      "Accuracy: 0.38992042440318303\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5208333333333334\n",
      "  Recall:    0.704225352112676\n",
      "  F1:        0.5988023952095808\n",
      "Happy:\n",
      "  Precision: 0.4017094017094017\n",
      "  Recall:    0.44339622641509435\n",
      "  F1:        0.42152466367713004\n",
      "Relaxed:\n",
      "  Precision: 0.2894736842105263\n",
      "  Recall:    0.32673267326732675\n",
      "  F1:        0.30697674418604654\n",
      "Sad:\n",
      "  Precision: 0.34\n",
      "  Recall:    0.1717171717171717\n",
      "  F1:        0.22818791946308725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec Logistic Regression: Position-based Evaluation (NO retraining) ===\")\n",
    "\n",
    "# START\n",
    "dev_start_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_start_texts]\n",
    "w2v_start_preds = lr_w2v.predict(dev_start_vecs)\n",
    "print(\"\\n--- W2V on START segment only ---\")\n",
    "print_results(dev_labels, w2v_start_preds)\n",
    "\n",
    "# MIDDLE\n",
    "dev_middle_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_middle_texts]\n",
    "w2v_middle_preds = lr_w2v.predict(dev_middle_vecs)\n",
    "print(\"\\n--- W2V on MIDDLE segment only ---\")\n",
    "print_results(dev_labels, w2v_middle_preds)\n",
    "\n",
    "# END\n",
    "dev_end_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_end_texts]\n",
    "w2v_end_preds = lr_w2v.predict(dev_end_vecs)\n",
    "print(\"\\n--- W2V on END segment only ---\")\n",
    "print_results(dev_labels, w2v_end_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a5a0d",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8275c4c9-0c1f-4969-a64a-e055131ce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length: int = 256):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(_tok, batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def train_and_eval_transformer(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    dev_dataset: Dataset,\n",
    "    output_dir: str,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    train_bs: int,\n",
    "    eval_bs: int,\n",
    "    set_pad_token_eos: bool = False,\n",
    "):\n",
    "    print(f\"=== Fine-tuning {model_name} ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenized_train = tokenize_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenize_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(EMOTIONS),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    if set_pad_token_eos:\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_bs,\n",
    "        per_device_eval_batch_size=eval_bs,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=16,\n",
    "        log_level=\"error\",\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_dev,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"{model_name} dev results:\", eval_results)\n",
    "\n",
    "    pred_output = trainer.predict(tokenized_dev)\n",
    "    logits = pred_output.predictions\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    print(f\"{model_name} classification report:\")\n",
    "    print_results(dev_labels, pred_labels)\n",
    "\n",
    "    return trainer, eval_results, pred_labels\n",
    "\n",
    "def eval_transformer_on_segments(model_name: str, trainer: Trainer):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned transformer (trainer) on START/MIDDLE/END\n",
    "    segments of the dev set, without retraining.\n",
    "    \"\"\"\n",
    "    tokenizer = trainer.tokenizer\n",
    "\n",
    "    for seg in [\"start\", \"middle\", \"end\"]:\n",
    "        # Build segmented dev texts\n",
    "        seg_texts = [get_segment(t, seg) for t in dev_texts]\n",
    "\n",
    "        # Build a segmented dev Dataset with same labels\n",
    "        seg_dev_ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"text\": seg_texts,\n",
    "                \"label\": [label2id[l] for l in dev_labels],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Tokenize segmented dataset\n",
    "        tokenized_seg_dev = tokenize_dataset(seg_dev_ds, tokenizer)\n",
    "\n",
    "        # Predict\n",
    "        pred_output = trainer.predict(tokenized_seg_dev)\n",
    "        logits = pred_output.predictions\n",
    "        pred_ids = np.argmax(logits, axis=-1)\n",
    "        pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "        print(f\"\\n=== {model_name} on {seg.upper()} segment only ===\")\n",
    "        print_results(dev_labels, pred_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02eb96",
   "metadata": {},
   "source": [
    "## DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc595940-367c-40d2-96c0-8cc36c91b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilgpt2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 1184.45 examples/s]\n",
      "Map: 100%|██████████| 377/377 [00:00<00:00, 1583.07 examples/s]\n",
      "C:\\Users\\samke\\AppData\\Local\\Temp\\ipykernel_23332\\3352690810.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3599, 'grad_norm': 29.54327964782715, 'learning_rate': 4.85e-05, 'epoch': 0.16}\n",
      "{'loss': 1.8893, 'grad_norm': 92.1890640258789, 'learning_rate': 4.69e-05, 'epoch': 0.32}\n",
      "{'loss': 1.5923, 'grad_norm': 33.16315841674805, 'learning_rate': 4.53e-05, 'epoch': 0.48}\n",
      "{'loss': 1.428, 'grad_norm': 21.53084373474121, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4135, 'grad_norm': 29.720989227294922, 'learning_rate': 4.21e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3911, 'grad_norm': 48.5584602355957, 'learning_rate': 4.05e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 1.3949381113052368, 'eval_precision': 0.3240610760433206, 'eval_recall': 0.3534003446850133, 'eval_f1': 0.31606123048252804, 'eval_accuracy': 0.35013262599469497, 'eval_runtime': 102.2248, 'eval_samples_per_second': 3.688, 'eval_steps_per_second': 0.929, 'epoch': 1.0}\n",
      "{'loss': 1.3335, 'grad_norm': 22.279226303100586, 'learning_rate': 3.8900000000000004e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2804, 'grad_norm': 43.611995697021484, 'learning_rate': 3.73e-05, 'epoch': 1.28}\n",
      "{'loss': 1.1981, 'grad_norm': 28.258705139160156, 'learning_rate': 3.57e-05, 'epoch': 1.44}\n",
      "{'loss': 1.3271, 'grad_norm': 22.173561096191406, 'learning_rate': 3.41e-05, 'epoch': 1.6}\n",
      "{'loss': 1.3812, 'grad_norm': 42.61476516723633, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.76}\n",
      "{'loss': 1.3247, 'grad_norm': 27.724191665649414, 'learning_rate': 3.09e-05, 'epoch': 1.92}\n",
      "{'eval_loss': 1.3562989234924316, 'eval_precision': 0.4196072641058268, 'eval_recall': 0.38642759439377367, 'eval_f1': 0.31920200543017296, 'eval_accuracy': 0.35013262599469497, 'eval_runtime': 89.4647, 'eval_samples_per_second': 4.214, 'eval_steps_per_second': 1.062, 'epoch': 2.0}\n",
      "{'loss': 1.1871, 'grad_norm': 37.43473434448242, 'learning_rate': 2.93e-05, 'epoch': 2.08}\n",
      "{'loss': 1.1019, 'grad_norm': 23.980382919311523, 'learning_rate': 2.7700000000000002e-05, 'epoch': 2.24}\n",
      "{'loss': 1.0392, 'grad_norm': 29.59819221496582, 'learning_rate': 2.61e-05, 'epoch': 2.4}\n",
      "{'loss': 1.0502, 'grad_norm': 14.180225372314453, 'learning_rate': 2.45e-05, 'epoch': 2.56}\n",
      "{'loss': 1.0079, 'grad_norm': 44.99965286254883, 'learning_rate': 2.29e-05, 'epoch': 2.7199999999999998}\n",
      "{'loss': 1.0384, 'grad_norm': 22.93215560913086, 'learning_rate': 2.13e-05, 'epoch': 2.88}\n",
      "{'eval_loss': 1.4146356582641602, 'eval_precision': 0.41849599179716634, 'eval_recall': 0.44053058640969067, 'eval_f1': 0.3932744045345973, 'eval_accuracy': 0.40848806366047746, 'eval_runtime': 86.4759, 'eval_samples_per_second': 4.36, 'eval_steps_per_second': 1.099, 'epoch': 3.0}\n",
      "{'loss': 1.0546, 'grad_norm': 15.726054191589355, 'learning_rate': 1.97e-05, 'epoch': 3.04}\n",
      "{'loss': 0.7737, 'grad_norm': 23.98394203186035, 'learning_rate': 1.81e-05, 'epoch': 3.2}\n",
      "{'loss': 0.786, 'grad_norm': 35.259498596191406, 'learning_rate': 1.65e-05, 'epoch': 3.36}\n",
      "{'loss': 0.8078, 'grad_norm': 56.7092399597168, 'learning_rate': 1.49e-05, 'epoch': 3.52}\n",
      "{'loss': 0.9642, 'grad_norm': 15.93453311920166, 'learning_rate': 1.3300000000000001e-05, 'epoch': 3.68}\n",
      "{'loss': 0.746, 'grad_norm': 25.23532485961914, 'learning_rate': 1.1700000000000001e-05, 'epoch': 3.84}\n",
      "{'loss': 0.7793, 'grad_norm': 25.0000057220459, 'learning_rate': 1.0100000000000002e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 1.362322211265564, 'eval_precision': 0.5057898769920005, 'eval_recall': 0.5232609495336991, 'eval_f1': 0.5019385914233538, 'eval_accuracy': 0.506631299734748, 'eval_runtime': 89.8248, 'eval_samples_per_second': 4.197, 'eval_steps_per_second': 1.058, 'epoch': 4.0}\n",
      "{'loss': 0.5655, 'grad_norm': 13.878230094909668, 'learning_rate': 8.500000000000002e-06, 'epoch': 4.16}\n",
      "{'loss': 0.4272, 'grad_norm': 38.511234283447266, 'learning_rate': 6.900000000000001e-06, 'epoch': 4.32}\n",
      "{'loss': 0.5822, 'grad_norm': 30.02216148376465, 'learning_rate': 5.3e-06, 'epoch': 4.48}\n",
      "{'loss': 0.6309, 'grad_norm': 76.43771362304688, 'learning_rate': 3.7e-06, 'epoch': 4.64}\n",
      "{'loss': 0.5644, 'grad_norm': 20.329696655273438, 'learning_rate': 2.1000000000000002e-06, 'epoch': 4.8}\n",
      "{'loss': 0.4947, 'grad_norm': 15.097702980041504, 'learning_rate': 5.000000000000001e-07, 'epoch': 4.96}\n",
      "{'eval_loss': 1.4092812538146973, 'eval_precision': 0.4664785057551374, 'eval_recall': 0.4875328085559021, 'eval_f1': 0.4709718976603193, 'eval_accuracy': 0.46949602122015915, 'eval_runtime': 88.0023, 'eval_samples_per_second': 4.284, 'eval_steps_per_second': 1.08, 'epoch': 5.0}\n",
      "{'train_runtime': 2502.1311, 'train_samples_per_second': 0.799, 'train_steps_per_second': 0.2, 'train_loss': 1.0771934609413147, 'epoch': 5.0}\n",
      "{'eval_loss': 1.4092812538146973, 'eval_precision': 0.4664785057551374, 'eval_recall': 0.4875328085559021, 'eval_f1': 0.4709718976603193, 'eval_accuracy': 0.46949602122015915, 'eval_runtime': 100.2508, 'eval_samples_per_second': 3.761, 'eval_steps_per_second': 0.948, 'epoch': 5.0}\n",
      "distilgpt2 dev results: {'eval_loss': 1.4092812538146973, 'eval_precision': 0.4664785057551374, 'eval_recall': 0.4875328085559021, 'eval_f1': 0.4709718976603193, 'eval_accuracy': 0.46949602122015915, 'eval_runtime': 100.2508, 'eval_samples_per_second': 3.761, 'eval_steps_per_second': 0.948, 'epoch': 5.0}\n",
      "distilgpt2 classification report:\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4664785057551374\n",
      "Recall: 0.4875328085559021\n",
      "F1: 0.4709718976603193\n",
      "Accuracy: 0.46949602122015915\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5652173913043478\n",
      "  Recall:    0.7323943661971831\n",
      "  F1:        0.6380368098159509\n",
      "Happy:\n",
      "  Precision: 0.4715447154471545\n",
      "  Recall:    0.5471698113207547\n",
      "  F1:        0.5065502183406113\n",
      "Relaxed:\n",
      "  Precision: 0.4246575342465753\n",
      "  Recall:    0.3069306930693069\n",
      "  F1:        0.3563218390804598\n",
      "Sad:\n",
      "  Precision: 0.4044943820224719\n",
      "  Recall:    0.36363636363636365\n",
      "  F1:        0.3829787234042553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_trainer, gpt2_results, gpt2_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilgpt2\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilgpt2_output\",\n",
    "    num_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4e43",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe4e592e-e7ad-4d23-b757-580470945074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fine-tuning distilbert/distilbert-base-uncased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 1531.62 examples/s]\n",
      "Map: 100%|██████████| 377/377 [00:00<00:00, 2254.85 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\samke\\AppData\\Local\\Temp\\ipykernel_23332\\3352690810.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 25:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.375800</td>\n",
       "      <td>1.303551</td>\n",
       "      <td>0.356097</td>\n",
       "      <td>0.485590</td>\n",
       "      <td>0.407268</td>\n",
       "      <td>0.458886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.179700</td>\n",
       "      <td>1.196877</td>\n",
       "      <td>0.475669</td>\n",
       "      <td>0.501320</td>\n",
       "      <td>0.434963</td>\n",
       "      <td>0.469496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>1.145496</td>\n",
       "      <td>0.535450</td>\n",
       "      <td>0.559977</td>\n",
       "      <td>0.543441</td>\n",
       "      <td>0.535809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilbert-base-uncased dev results: {'eval_loss': 1.1454964876174927, 'eval_precision': 0.5354502471523748, 'eval_recall': 0.559977496553801, 'eval_f1': 0.5434413100952952, 'eval_accuracy': 0.5358090185676393, 'eval_runtime': 96.5857, 'eval_samples_per_second': 3.903, 'eval_steps_per_second': 0.984, 'epoch': 3.0}\n",
      "distilbert/distilbert-base-uncased classification report:\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.5354502471523748\n",
      "Recall: 0.559977496553801\n",
      "F1: 0.5434413100952952\n",
      "Accuracy: 0.5358090185676393\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.648936170212766\n",
      "  Recall:    0.8591549295774648\n",
      "  F1:        0.7393939393939394\n",
      "Happy:\n",
      "  Precision: 0.5252525252525253\n",
      "  Recall:    0.49056603773584906\n",
      "  F1:        0.5073170731707317\n",
      "Relaxed:\n",
      "  Precision: 0.4888888888888889\n",
      "  Recall:    0.43564356435643564\n",
      "  F1:        0.4607329842931937\n",
      "Sad:\n",
      "  Precision: 0.4787234042553192\n",
      "  Recall:    0.45454545454545453\n",
      "  F1:        0.46632124352331605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distilbert_trainer, distilbert_results, distilbert_pred_labels = train_and_eval_transformer(\n",
    "    model_name=\"distilbert/distilbert-base-uncased\",\n",
    "    train_dataset=train_ds,\n",
    "    dev_dataset=dev_ds,\n",
    "    output_dir=\"./distilbert_musicmood\",\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    train_bs=4,\n",
    "    eval_bs=4,\n",
    "    set_pad_token_eos=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b68de",
   "metadata": {},
   "source": [
    "## Segment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d96d64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 1679.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilGPT2 on START segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.4040863872739266\n",
      "Recall: 0.3989931347639149\n",
      "F1: 0.3790618725102379\n",
      "Accuracy: 0.3793103448275862\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6027397260273972\n",
      "  Recall:    0.6197183098591549\n",
      "  F1:        0.6111111111111112\n",
      "Happy:\n",
      "  Precision: 0.42105263157894735\n",
      "  Recall:    0.3018867924528302\n",
      "  F1:        0.3516483516483517\n",
      "Relaxed:\n",
      "  Precision: 0.3\n",
      "  Recall:    0.1188118811881188\n",
      "  F1:        0.1702127659574468\n",
      "Sad:\n",
      "  Precision: 0.2925531914893617\n",
      "  Recall:    0.5555555555555556\n",
      "  F1:        0.3832752613240418\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 2803.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilGPT2 on MIDDLE segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.46718459839149495\n",
      "Recall: 0.47073696540528354\n",
      "F1: 0.450027881236454\n",
      "Accuracy: 0.4509283819628647\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.6533333333333333\n",
      "  Recall:    0.6901408450704225\n",
      "  F1:        0.6712328767123288\n",
      "Happy:\n",
      "  Precision: 0.527027027027027\n",
      "  Recall:    0.36792452830188677\n",
      "  F1:        0.43333333333333335\n",
      "Relaxed:\n",
      "  Precision: 0.3148148148148148\n",
      "  Recall:    0.16831683168316833\n",
      "  F1:        0.21935483870967742\n",
      "Sad:\n",
      "  Precision: 0.3735632183908046\n",
      "  Recall:    0.6565656565656566\n",
      "  F1:        0.47619047619047616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 3696.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilGPT2 on END segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.42503704628704625\n",
      "Recall: 0.4216326828005681\n",
      "F1: 0.41430704626535536\n",
      "Accuracy: 0.40318302387267907\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.5769230769230769\n",
      "  Recall:    0.6338028169014085\n",
      "  F1:        0.6040268456375839\n",
      "Happy:\n",
      "  Precision: 0.44155844155844154\n",
      "  Recall:    0.32075471698113206\n",
      "  F1:        0.37158469945355194\n",
      "Relaxed:\n",
      "  Precision: 0.375\n",
      "  Recall:    0.26732673267326734\n",
      "  F1:        0.31213872832369943\n",
      "Sad:\n",
      "  Precision: 0.30666666666666664\n",
      "  Recall:    0.46464646464646464\n",
      "  F1:        0.36947791164658633\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 2446.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on START segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.5102223594065909\n",
      "Recall: 0.49575194697253405\n",
      "F1: 0.4983741388589917\n",
      "Accuracy: 0.48010610079575594\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.7164179104477612\n",
      "  Recall:    0.676056338028169\n",
      "  F1:        0.6956521739130435\n",
      "Happy:\n",
      "  Precision: 0.5263157894736842\n",
      "  Recall:    0.37735849056603776\n",
      "  F1:        0.43956043956043955\n",
      "Relaxed:\n",
      "  Precision: 0.4375\n",
      "  Recall:    0.48514851485148514\n",
      "  F1:        0.460093896713615\n",
      "Sad:\n",
      "  Precision: 0.36065573770491804\n",
      "  Recall:    0.4444444444444444\n",
      "  F1:        0.39819004524886875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 1037.54 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on MIDDLE segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.5329355898019736\n",
      "Recall: 0.5139028983950746\n",
      "F1: 0.5186916469487651\n",
      "Accuracy: 0.4960212201591512\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.7536231884057971\n",
      "  Recall:    0.7323943661971831\n",
      "  F1:        0.7428571428571429\n",
      "Happy:\n",
      "  Precision: 0.6233766233766234\n",
      "  Recall:    0.4528301886792453\n",
      "  F1:        0.5245901639344263\n",
      "Relaxed:\n",
      "  Precision: 0.3888888888888889\n",
      "  Recall:    0.4158415841584158\n",
      "  F1:        0.4019138755980861\n",
      "Sad:\n",
      "  Precision: 0.36585365853658536\n",
      "  Recall:    0.45454545454545453\n",
      "  F1:        0.40540540540540543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 377/377 [00:00<00:00, 3232.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistilBERT on END segment only ===\n",
      "=== Overall (Macro Avg) ===\n",
      "Precision: 0.5313176355256792\n",
      "Recall: 0.5292664431205278\n",
      "F1: 0.5297445809428624\n",
      "Accuracy: 0.5145888594164456\n",
      "\n",
      "=== Per Emotion (Class) Metrics ===\n",
      "Angry:\n",
      "  Precision: 0.7083333333333334\n",
      "  Recall:    0.7183098591549296\n",
      "  F1:        0.7132867132867133\n",
      "Happy:\n",
      "  Precision: 0.5789473684210527\n",
      "  Recall:    0.5188679245283019\n",
      "  F1:        0.5472636815920398\n",
      "Relaxed:\n",
      "  Precision: 0.42452830188679247\n",
      "  Recall:    0.44554455445544555\n",
      "  F1:        0.43478260869565216\n",
      "Sad:\n",
      "  Precision: 0.41346153846153844\n",
      "  Recall:    0.43434343434343436\n",
      "  F1:        0.4236453201970443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_transformer_on_segments(\"DistilGPT2\", gpt2_trainer)\n",
    "eval_transformer_on_segments(\"DistilBERT\", distilbert_trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
